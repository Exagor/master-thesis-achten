{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47317110",
   "metadata": {},
   "source": [
    "# Gemma models testing\n",
    "\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca68e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 14:35:15,207 CropBox missing from /Page, defaulting to MediaBox\n",
      "2025-05-23 14:35:15,209 CropBox missing from /Page, defaulting to MediaBox\n",
      "2025-05-23 14:35:15,210 CropBox missing from /Page, defaulting to MediaBox\n",
      "2025-05-23 14:35:15,612 CropBox missing from /Page, defaulting to MediaBox\n",
      "2025-05-23 14:35:15,613 CropBox missing from /Page, defaulting to MediaBox\n",
      "2025-05-23 14:35:15,614 CropBox missing from /Page, defaulting to MediaBox\n",
      "2025-05-23 14:35:15,615 Text extracted from the PDF file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['24EM03456'])\n",
      "{'24EM03456': {'text': \"COPIE INTERNE 29/10/2024\\nDr ROGER Thierry\\nCHIREC DELTA\\nRadiodiagnostic\\nCentre d’Anatomie BOULEVARD DU TRIOMPHE 1\\nPathologique H.U.B. 1160 AUDERGHEM\\nRue Meylemeersch 90 - 1070 Anderlecht\\nMijlemeerschstraat 90 – 1070 Anderlecht\\nPATIENT :\\nDirectrice de Service\\nPr Myriam Remmelink\\nID :\\nRéf. Externe : 24CU052383 EXAMEN : 24EM03456\\nEquipe Médicale\\nDr Nicolas de Saint Aubain\\nPr Nicky D’Haene Prélevé le 30/07/2024 à 30/07/2024 11:30 Prescripteur : Dr ROGER Thierry\\nDr Maria Gomez Galdon Reçu le 08/08/2024\\nDr Chirine Khaled\\nPr Denis Larsimont\\nDr Laetitia Lebrun\\nDr Calliope Maris\\nPr Jean-Christophe Noël\\nPr Isabelle Salmon\\nDr Anne-Laure Trépant\\nPr Laurine Verset\\nRECHERCHE PAR « NEXT GENERATION SEQUENCING » DE\\nConsultant (e) s\\nDr Sarah Bouri MUTATIONS DANS 25 GENES IMPLIQUES DANS LES CANCERS\\nDr Xavier Catteau PULMONAIRES, LES GIST ET MELANOMES\\nDr Roland de Wind\\nDr Valérie Segers (Colon and Lung Panel + Oncomine Solid Tumor-plus PANEL)\\nDr Anne Theunis\\nDr Marie-Paule Van Craynest\\nHUB – Centre d’Anatomie Pathologique – est accrédité par BELAC sous le numéro de\\ncertificat B-727 MED\\nSecrétariat Médical\\nT. +32 (0)2 541 73 23\\n+32 (0)2 555 33 35\\nI. Renseignements anatomopathologiques\\nSecMed.AnaPath@hubruxelles.be\\nSecrétariat Direction N° du prélèvement : 24CU052383 pneu\\nT. +32 (0)2 555 31 15\\nMme Kathia El Yassini\\nKathia.elyassini@hubruxelles.be Date du prélèvement : 30/07/2024\\nMme Véronique Millecamps\\nveronique.millecamps@hubruxelles.be Origine du prélèvement : Curepath\\nType de prélèvement : Adénocarcinome TTF1+\\nII. Evaluation de l’échantillon\\n- % de cellules tumorales : <10%\\n- Qualité du séquençage : Optimale (coverage moyen > 1000x)\\n- Les exons à considérer comme non contributifs sont détaillés dans le tableau ci-dessous (point III).\\n- Commentaires : /\\n1 / 3\\nSuite de l’examen N° 24EM03456 concernant le patient\\nIII. Méthodologie (effectué par : CLVA, NADN, NIDH)\\n- Extraction ADN à partir de coupes paraffinées après macrodissection des zones\\ntumorales ou à partir de frottis.\\n- Détection par « Next Generation Sequencing » (sur Ion Gene Studio S5, Ion Torrent\\navec Kit AmpliSeq colon & lung cancer panel et OST-plus) de mutations dans 25 gènes\\nliés aux cancers pulmonaires, GIST et mélanomes:\\nExons Non Exons Non\\nContributif Contributif\\nGene RefSeq Exons testés Gene RefSeq Exons testés\\n(coverage < (coverage <\\n250x)* 250x)*\\n8, 9, 11, 13, 14,\\nAKT1 NM_05163 3 KIT NM_000222 17, 18\\nALK NM_004304 22, 23, 24, 25 KRAS NM_033360 2-4\\nBRAF NM_004333 11, 15 MAP2K1 NM_002755 2\\nCTNNB1 NM_001904 3 MET NM_001127500 2, 14-20\\nDDR2 NM_001014796 6, 9, 13-16, 18 NOTCH1 NM_017617 26, 27\\nEGFR NM_005228 12, 18-21 NRAS NM_002524 2, 3, 4\\nERBB2 NM_004448 19-21 PDGFRA NM_006206 12, 14, 18\\n3, 4, 6-10, 12,\\nERBB4 NM_005235 15, 23 PIK3CA NM_006218 9, 13, 20\\nFBXW7 NM_033632 5, 8-11 8 PTEN NM_000314 1, 3, 6-8\\nFGFR1 NM_023110 4, 7 SMAD4 NM_005359 3, 5, 6, 8-10, 12\\nFGFR2 NM_022970 7, 9, 12, 14 STK11 NM_000455 1, 4-6, 8\\nFGFR3 NM_000142 7, 9, 14, 16, 18 TP53 NM_000546 2, 4-8, 10\\nHRAS NM_005343 2, 3, 4\\n* Un coverage < 250x induit une perte de sensibilité et de spécificité de la méthode.\\n- Sensibilité : la technique utilisée détecte une mutation si l’échantillon contient > 4%\\nd’ADN mutant. Seules les mutations rapportées dans COSMIC et avec une fréquence\\nsupérieure à 4% et un variant coverage >30x sont rapportées.\\nIV. Résultats\\nListe des mutations détectées :\\nGène Exon Mutation Coverage % d’ADN muté\\nMutations avec impact clinique potentiel\\nKRAS 3 p.A59E 1777 2%\\nMutations avec impact clinique indéterminé\\nTP53 7 p.P250L 1939 5%\\nLes données suggèrent la présence de la mutation A59E du gène KRAS à une\\nfréquence allélique de 2%.\\nThéoriquement, seules les mutations avec une fréquence allélique supérieure à 4%\\net un variant coverage >30x sont rapportées. Néanmoins, la mutation A59E du gène\\nKRAS a été retrouvée lors de 2 expériences indépendantes (avec une fréquence\\nallélique similaire), c'est pourquoi, elle est rapportée ici.\\nV. Discussion :\\nL'impact de la présence de mutation du gène KRAS sur la sensibilité ou la\\nrésistance aux inhibiteurs de la voie EGFR est indéterminé à ce jour. Il est à noter\\nque les mutations du gène KRAS sont mutuellement exclusives avec les mutations\\ndu gène EGFR et les translocations du gène ALK et du gène ROS1.\\nVincent MD et al., Curr Oncol 2013, 19 :s33-s44\\nLes mutations du gène TP53 sont fréquentes dans les cancers pulmonaires, leur impact\\nclinique est indéterminé.\\n2 / 3\\nSuite de l’examen N° 24EM03456 concernant le patient\\nVI. Conclusion : (CLVA le 20/08/2024)\\nAbsence de mutation détectée dans le gène EGFR.\\nAbsence de mutation détectée dans le codon V600 du gène BRAF.\\nPrésence de la mutation A59E du gène KRAS. Néanmoins la fréquence allélique est\\nen-dessous du seuil de détection validé (voir Résultats).\\nA noter la présence de la mutation P250L du gène TP53, dont l’impact clinique est\\nindéterminé.\\nCe résultat est à considérer avec précaution en raison du faible pourcentage de\\ncellules tumorales dans l’échantillon analysé.\\nEn raison de la présence d'une mutation driver (A59E du gène KRAS), la recherche d'un\\nréarrangement des gènes ALK, ROS1, RET, NTRK1, NTRK2 et NTRK3 ne sera pas\\neffectuée.\\nPour toute information complémentaire, veuillez nous contacter au 02/555.85.08 ou par mail :\\nBiomol.AnaPath@erasme.ulb.ac.be\\nN.B. Pour les prélèvements d’histologie et de cytologie ainsi que pour les examens complémentaires\\nde biologie moléculaire, merci d’utiliser les nouvelles prescriptions disponibles sur le site internet du\\nHUB :\\nhttps://www.hubruxelles.be/sites/default/files/2024-03-04_demande%20analyse%20anapath%20cytologie%20v3.pdf\\nhttps://www.hubruxelles.be/sites/default/files/FO-HUB-BM-11%20Demande%20de%20biologie%20mol%C3%A9culaire-\\nIPD%20v1.doc\\nDr N D'HAENE\\nDr COMPERE CHRISTOPHE\\n3 / 3\", 'image': [<PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=1653x2339 at 0x7F1127BB8430>, <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=1653x2339 at 0x7F1127BB86A0>, <PIL.PpmImagePlugin.PpmImageFile image mode=RGB size=1653x2339 at 0x7F1127BBBA90>]}}\n"
     ]
    }
   ],
   "source": [
    "from pdf_parser import *\n",
    "import os\n",
    "\n",
    "pdf_folder_path = \"../data/PDF\"\n",
    "# pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]\n",
    "pdf_files = [\"24EM03456.pdf\"]\n",
    "image_save_folder_path = \"../data/images\"\n",
    "\n",
    "pdf_text_image = {}\n",
    "for pdf_file in pdf_files:\n",
    "    #could use langchain here to extract text from pdf and use Document object\n",
    "    pdf_text_image[os.path.splitext(pdf_file)[0]] = {\"text\":extract_with_pdfplumber(os.path.join(pdf_folder_path,pdf_file))}\n",
    "    pdf_images = extract_pdf2image(f\"{pdf_folder_path}/{pdf_file}\")\n",
    "    pdf_text_image[os.path.splitext(pdf_file)[0]][\"image\"] = pdf_images\n",
    "\n",
    "print(pdf_text_image.keys())\n",
    "print(pdf_text_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc78c9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Extraction de données depuis un rapport de biopsie\n",
      "\n",
      "Tu es un assistant médical intelligent spécialisé en oncologie.  \n",
      "Ta mission est d’extraire de manière fiable et structurée les **informations médicales clés** à partir de **rapports de biopsie** provenant de services d'oncologie hospitaliers.\n",
      "\n",
      "Tu recevras un **document textuel**, ou le **texte OCR extrait d'une image de rapport**, contenant les informations nécessaires.  \n",
      "Ta sortie doit être un **dictionnaire JSON structuré** et exploitable.\n",
      "\n",
      "---\n",
      "\n",
      "## Objectifs\n",
      "\n",
      "1. Lire et comprendre le contenu d’un rapport de biopsie.\n",
      "2. Identifier et extraire les informations cliniques spécifiques listées ci-dessous.\n",
      "3. Structurer les informations extraites sous forme de dictionnaire conforme.\n",
      "\n",
      "---\n",
      "\n",
      "## Informations à extraire\n",
      "\n",
      "Pour chaque rapport, extrais les données suivantes :\n",
      "\n",
      "| Clé du dictionnaire       | Description                                                                 |\n",
      "|---------------------------|-----------------------------------------------------------------------------|\n",
      "| `\"Examen\"`                | Numéro d'examen (code unique du rapport)                                   |\n",
      "| `\"N° du prélèvement\"`     | Numéro du prélèvement ou de l’échantillon                                   |\n",
      "| `\"Panel\"`                 | Panel de séquençage utilisé, catégorisé comme suit :                        |\n",
      "|                           | - Colon et poumon → `\"CLP\"`                                                |\n",
      "|                           | - Oncomine Solid Tumor → `\"OST\"`                                           |\n",
      "|                           | - Ovaire, endomètre, sein → `\"GP\"`                                         |\n",
      "|                           | - Thyroïde → `\"TP\"`                                                        |\n",
      "|                           | - Autres cancers → `\"CHP\"`                                                 |\n",
      "| `\"Origine du prélèvement\"`| Lieu géographique ou établissement d’où provient le prélèvement             |\n",
      "| `\"Type de prélèvement\"`   | Nature du prélèvement ou type histologique (ex. : adénocarcinome)           |\n",
      "| `\"Qualité du séquencage\"` | Appréciation de la qualité du séquençage (ex. : Optimale, Sous-optimale)   |\n",
      "| `\"% cellules\"`            | Pourcentage de cellules tumorales analysées ou à analyser (exprimé en % sans le signe)   |\n",
      "\n",
      "---\n",
      "\n",
      "## Format de sortie attendu\n",
      "\n",
      "- Tu dois retourner un **dictionnaire JSON** contenant uniquement les clés listées ci-dessus.\n",
      "- Les clés doivent **être exactement identiques** à celles spécifiées.\n",
      "- Les valeurs doivent être extraites telles quelles, sans modification.\n",
      "- Exemple de sortie :\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Examen\": \"24EM03460\",\n",
      "  \"N° du prélèvement\": \"24MH9721 BN\",\n",
      "  \"Panel\": \"CLP\",\n",
      "  \"Origine du prélèvement\": \"Centre Hospitalier de Mouscron\",\n",
      "  \"Type de prélèvement\": \"Adénocarcinome lieberkühnien\",\n",
      "  \"Qualité du séquencage\": \"Optimale\",\n",
      "  \"% cellules\": 50\n",
      "}\n",
      "```\n",
      "\n",
      "## Contraintes et règles\n",
      "\n",
      "- Si une donnée est absente ou non mentionnée, indique \"None\" comme valeur.\n",
      "- Ne jamais inventer ou compléter des données non présentes.\n",
      "- Respecter strictement les termes tels qu'ils apparaissent dans le texte.\n",
      "\n",
      "# Extraction de données depuis un rapport de biopsie\n",
      "\n",
      "Tu es un **assistant médical intelligent spécialisé en oncologie moléculaire**.\n",
      "\n",
      "Ta mission est d’**extraire automatiquement** les **informations médicales structurées** à partir de rapports de biopsie provenant de services hospitaliers. Ces documents peuvent être fournis sous forme de texte brut ou de texte OCR issu d'une image.\n",
      "\n",
      "---\n",
      "\n",
      "## Objectif principal\n",
      "\n",
      "Analyser le contenu d’un **rapport de biopsie** et **retourner une liste de dictionnaires JSON** contenant **uniquement les données moléculaires pertinentes** extraites de la **section \"Résultats\"**.\n",
      "\n",
      "---\n",
      "\n",
      "## Informations à extraire\n",
      "\n",
      "Pour chaque **mutation détectée**, extrais les éléments suivants :\n",
      "\n",
      "| Clé du dictionnaire       | Description                                                                 |\n",
      "|---------------------------|-----------------------------------------------------------------------------|\n",
      "| `\"Examen\"`                | Numéro ou code unique de l'examen figurant dans le rapport                  |\n",
      "| `\"Gène\"`                  | Nom du gène dans lequel une mutation est identifiée                         |\n",
      "| `\"Exon\"`                  | Numéro de l’exon concerné par la mutation                                   |\n",
      "| `\"Mutation\"`              | Description de la mutation détectée                                         |\n",
      "| `\"Coverage\"`              | Couverture de lecture (coverage) pour cette mutation                        |\n",
      "| `\"% d'ADN muté\"`          | Pourcentage d’ADN mutant détecté pour cette mutation                        |\n",
      "| `\"Impact clinique\"`       | Impact clinique rapporté (valeurs possibles : `\"Avéré\"`, `\"Potentiel\"`, `\"Indéterminé\"`) |\n",
      "\n",
      "### Cas particulier : aucun variant détecté\n",
      "\n",
      "Si la section \"Résultats\" n’indique **aucune mutation détectée**, retourner la structure suivante :\n",
      "\n",
      "```json\n",
      "[{\n",
      "    \"Examen\": \"None\",\n",
      "    \"Gène\": \"None\",\n",
      "    \"Exon\": \"None\",\n",
      "    \"Mutation\": \"None\",\n",
      "    \"Coverage\": \"None\",\n",
      "    \"% d'ADN muté\": \"None\",\n",
      "    \"Impact clinique\": \"None\"\n",
      "}]\n",
      "```\n",
      "\n",
      "## Instructions de traitement\n",
      "\n",
      "1. Analyser uniquement la section \"Résultats\" du rapport.\n",
      "2. Extraire les données telles qu’elles apparaissent dans le texte. Aucune interprétation ou reformulation n’est autorisée.\n",
      "3. Si une information attendue est absente ou non lisible, indiquer \"None\" comme valeur.\n",
      "4. Respecter strictement les noms de clés JSON fournis (casse, accents, orthographe).\n",
      "\n",
      "## Format de sortie attendu\n",
      "\n",
      "Une liste JSON, où chaque objet représente une mutation identifiée.\n",
      "\n",
      "Exemple :\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"Examen\": \"24EM03355\",\n",
      "        \"Gène\": \"BRAF\",\n",
      "        \"Exon\": \"15\",\n",
      "        \"Mutation\": \"p.N581I\",\n",
      "        \"Coverage\": \"1561\",\n",
      "        \"% d'ADN muté\": \"17\",\n",
      "        \"Impact clinique\": \"Potentiel\"\n",
      "    },\n",
      "    {\n",
      "        \"Examen\": \"24EM03355\",\n",
      "        \"Gène\": \"STK11\",\n",
      "        \"Exon\": \"6\",\n",
      "        \"Mutation\": \"p.P281Rfs*6\",\n",
      "        \"Coverage\": \"249\",\n",
      "        \"% d'ADN muté\": \"24\",\n",
      "        \"Impact clinique\": \"Indéterminé\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "## Contraintes\n",
      "\n",
      "- Ne jamais inventer ou déduire une donnée.\n",
      "- Ne pas inclure d’autres informations que celles spécifiées.\n",
      "- Ne pas reformater les valeurs (ex : laisser les pourcentages sous forme de chaînes numériques sans symbole % si c’est le cas dans le texte).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../prompt/system_prompt_metadata.md\", \"r\") as f:\n",
    "    system_prompt_meta = f.read()\n",
    "print(system_prompt_meta)\n",
    "\n",
    "with open(\"../prompt/system_prompt_mutation.md\", \"r\") as f:\n",
    "    system_prompt_mut = f.read()\n",
    "print(system_prompt_mut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f620a20",
   "metadata": {},
   "source": [
    "## Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "with open(\"../login.txt\", \"r\") as f:\n",
    "    token = f.read()\n",
    "login(token) #token from huggingface.co necessary to use gemma3\n",
    "print(\"login done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b03ba7",
   "metadata": {},
   "source": [
    "### Gemma 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b22502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [13:12<00:00, 396.25s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline initialized\n",
      "```json\n",
      "{\n",
      "  \"Examen\": \"24EM03456\",\n",
      "  \"N° du prélèvement\": \"24CU052383\",\n",
      "  \"Panel\": \"CLP\",\n",
      "  \"Origine du prélèvement\": \"Curepath\",\n",
      "  \"Type de prélèvement\": \"Adénocarcinome TTF1+\",\n",
      "  \"Qualité du séquencage\": \"Optimale\",\n",
      "  \"% cellules\": \"10\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"google/gemma-3-4b-it\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", #\"image-text-to-text\"\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=device, #uses \"cpu\" here because the \"cuda\" requires 3 Go more of VRAM (5GO total)\n",
    ")\n",
    "print(\"pipeline initialized\")\n",
    "\n",
    "messages_meta = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": system_prompt_meta}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            [{\"type\": \"text\", \"text\": pdf_text_image[\"24EM03456\"][\"text\"]}]\n",
    "            #+ [{\"type\": \"image\", \"image\": img} for img in pdf_text_image[\"24EM03352\"][\"image\"]]\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "messages_mut = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": system_prompt_mut}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            [{\"type\": \"text\", \"text\": pdf_text_image[\"24EM03456\"][\"text\"]}]\n",
    "            #+ [{\"type\": \"image\", \"image\": img} for img in pdf_text_image[\"24EM03352\"][\"image\"]]\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "output_gemma_4B = pipe(messages_meta, max_new_tokens=250) #temperature=0\n",
    "print(output_gemma_4B[0][\"generated_text\"][-1][\"content\"])\n",
    "output_gemma_4B2 = pipe(messages_mut, max_new_tokens=250) #temperature=0\n",
    "print(output_gemma_4B2[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8069b",
   "metadata": {},
   "source": [
    "\n",
    "### Medgemma 3\n",
    "When running on cpu, 38min inference time for 1 request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe3bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.69it/s]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline initialized\n",
      "```json\n",
      "{\n",
      "  \"Examen\": \"24EM03456\",\n",
      "  \"N° du prélèvement\": \"24CU052383 pneu\",\n",
      "  \"Panel\": \"CLP\",\n",
      "  \"Origine du prélèvement\": \"Curepath\",\n",
      "  \"Type de prélèvement\": \"Adénocarcinome TTF1+\",\n",
      "  \"Qualité du séquencage\": \"Optimale\",\n",
      "  \"% cellules\": \"2\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"google/medgemma-4b-it\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", #\"image-text-to-text\"\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=device, #uses \"cpu\" here because the \"cuda\" requires 3 Go more of VRAM (5GO total)\n",
    ")\n",
    "print(\"pipeline initialized\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": system_prompt_meta}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            [{\"type\": \"text\", \"text\": pdf_text_image[\"24EM03456\"][\"text\"]}]\n",
    "            #+ [{\"type\": \"image\", \"image\": img} for img in pdf_text_image[\"24EM03352\"][\"image\"]]\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "output_medgemma = pipe(messages, max_new_tokens=250) #temperature=0\n",
    "print(output_medgemma[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fdf310",
   "metadata": {},
   "source": [
    "### Gemma3 1B\n",
    "6 minutes for inference on 1 request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb16e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline initialized\n",
      "```json\n",
      "{\n",
      "  \"Examen\": \"24EM03456\",\n",
      "  \"N° du prélèvement\": \"24MH9721 BN\",\n",
      "  \"Panel\": \"CLP\",\n",
      "  \"Origine du prélèvement\": \"Centre Hospitalier de Mouscron\",\n",
      "  \"Type de prélèvement\": \"Adénocarcinome lieberkühnien\",\n",
      "  \"Qualité du séquencage\": \"Optimale\",\n",
      "  \"% cellules\": 50\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\" # google gemma small\n",
    "# model_name = \"google/gemma-3-4b-it-qat-q4_0-gguf\" #quantization form of the model\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=device, #uses \"cpu\" here because the \"cuda\" requires 3 Go more of VRAM (5GO total)\n",
    ")\n",
    "print(\"pipeline initialized\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": system_prompt_meta}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            [{\"type\": \"text\", \"text\": pdf_text_image[\"24EM03456\"][\"text\"]}]\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# the max_new_tokens is related to the time spent to generate the answer\n",
    "output_gemma_1B = pipe(messages, max_new_tokens=250) #can add temperature=0\n",
    "print(output_gemma_1B[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c67203",
   "metadata": {},
   "source": [
    "### Gemma3 4B quantized\n",
    "Quantized models can't be used with hugging face library.\n",
    "It need llama.cpp or ollama to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03432ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login done\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model google/gemma-3-4b-it-qat-q4_0-gguf with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 279, in _wrapper\n    return func(*args, **kwargs)\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4260, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1100, in _get_resolved_checkpoint_files\n    raise EnvironmentError(\nOSError: google/gemma-3-4b-it-qat-q4_0-gguf does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 574, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.gemma.configuration_gemma.GemmaConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogin done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-3-4b-it-qat-q4_0-gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#quantization form of the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#uses \"cpu\" here because the \"cuda\" requires 3 Go more of VRAM (5GO total)\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     {\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     }\n\u001b[1;32m     31\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/__init__.py:942\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 942\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    953\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:304\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    303\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model google/gemma-3-4b-it-qat-q4_0-gguf with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 279, in _wrapper\n    return func(*args, **kwargs)\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4260, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1100, in _get_resolved_checkpoint_files\n    raise EnvironmentError(\nOSError: google/gemma-3-4b-it-qat-q4_0-gguf does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/alexa/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 574, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.gemma.configuration_gemma.GemmaConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\n\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/gemma-3-4b-it-qat-q4_0-gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0df89e",
   "metadata": {},
   "source": [
    "Show results from experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236da04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m answer_meta \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m cleaned_meta \u001b[38;5;241m=\u001b[39m answer_meta\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;66;03m#clean the answer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(cleaned_meta)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "#save the output in json format\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "answer_meta = output[0][\"generated_text\"][-1][\"content\"]\n",
    "cleaned_meta = answer_meta.replace(\"```json\", \"\").replace(\"```\", \"\").strip() #clean the answer\n",
    "data = json.loads(cleaned_meta)\n",
    "print(data)\n",
    "\n",
    "answer_mut = output2[0][\"generated_text\"][-1][\"content\"]\n",
    "cleaned_mut = answer_mut.replace(\"```json\", \"\").replace(\"```\", \"\").strip() #clean the answer\n",
    "data2 = json.loads(cleaned_mut)\n",
    "print(data2)\n",
    "\n",
    "df = pd.DataFrame([data])\n",
    "df.to_excel(\"../out/metadata_gemma3_4B.xlsx\", index=False)\n",
    "df2 = pd.DataFrame(data2)\n",
    "df2.to_excel(\"../out/mutation_gemma3_4B.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
