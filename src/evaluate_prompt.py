"""
Script to evaluate prompts generated by other LLMs
Here the LLM is a scoring model that evaluates the prompts
"""

from transformers import AutoProcessor, Gemma3ForConditionalGeneration
import torch
from pdf_parser import *
import os
import torch.nn.functional as F

#get the prompt
with open("prompt/generated_prompt_metadata_gpt4o_merged.txt", "r") as f: #to test gpt4o, gpto3 and gemini
    system_prompt_meta = f.read()
with open("prompt/generated_prompt_mutation_gpt4o_merged.txt", "r") as f:
    system_prompt_mut = f.read()

# Get example input (text+image)
pdf_folder_path = "../data/PDF"
pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]
# pdf_files = ["24EM03456.pdf"]

pdf_text_image = {}
for pdf_file in pdf_files:
    pdf_text_image[os.path.splitext(pdf_file)[0]] = {"text":extract_with_pdfplumber(os.path.join(pdf_folder_path,pdf_file))}
    pdf_images = extract_pdf2image(f"{pdf_folder_path}/{pdf_file}")
    pdf_text_image[os.path.splitext(pdf_file)[0]]["image"] = pdf_images


# Create the model and processor
model_id = "google/gemma-3-4b-it"

model = Gemma3ForConditionalGeneration.from_pretrained(
    model_id, device_map="auto"
)

processor = AutoProcessor.from_pretrained(model_id)

messages_meta = [
    {
        "role": "system",
        "content": [{"type": "text", "text": system_prompt_meta}]
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": pdf_text_image["24EM03456"]["text"]}
            + [{"type": "image", "image": img} for img in pdf_text_image["24EM03456"]["image"]]
        ]
    }
]

messages_mut = [
    {
        "role": "system",
        "content": [{"type": "text", "text": system_prompt_mut}]
    },
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg"},
            {"type": "text", "text": "Describe this image in detail."}
        ]
    }
]

inputs = processor.apply_chat_template(
    messages_meta, add_generation_prompt=True, tokenize=True,
    return_dict=True, return_tensors="pt"
).to(model.device, dtype=torch.bfloat16)

input_len = inputs["input_ids"].shape[-1]

with torch.inference_mode(): #inference mode is similar to nograds param
    generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False) #the dosample=False is to get deterministic results
    # Remove the prompt part to get only generated tokens
    answer_ids = generated_ids[0, input_len:]

# 2. Concatenate prompt and generated tokens
full_ids = torch.cat([inputs["input_ids"][0], answer_ids], dim=0).unsqueeze(0)  # shape: (1, prompt+answer_len)

# 3. Forward pass to get logits for the full sequence
with torch.inference_mode():
    outputs = model(input_ids=full_ids)
    logits = outputs.logits  # shape: (1, seq_len, vocab_size)

# 4. Compute log-softmax over the logits
logprobs = F.log_softmax(logits, dim=-1)  # shape: (1, seq_len, vocab_size)

# 5. Gather logprobs for the generated tokens (the answer)
# For each generated token, get the logprob assigned by the model at the previous position
answer_logprobs = []
for i in range(input_len, full_ids.shape[1] - 1):
    token_id = full_ids[0, i + 1]
    logprob = logprobs[0, i, token_id]
    answer_logprobs.append(logprob.item())

# 6. Compute the average logprob for the generated answer
average_logprob = sum(answer_logprobs) / len(answer_logprobs) if answer_logprobs else float('-inf')
print(f"Average logprob for the generated answer: {average_logprob}")
# Decode the generated answer
decoded = processor.decode(generated_ids[0, input_len:], skip_special_tokens=True)
print(decoded)


# Save the logprob and decoded answer to a file
output_file = "out/evaluation_results.txt"
with open(output_file, "w") as f:
    f.write(f"Average logprob: {average_logprob}\n")
    f.write(f"Decoded answer: {decoded}\n")





######################################################
# """
# Same with qwen2.5VL

# """
# from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
# from qwen_vl_utils import process_vision_info

# # default: Load the model on the available device(s)
# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
#     "Qwen/Qwen2.5-VL-3B-Instruct", torch_dtype="auto", device_map="auto"
# )

# # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
# #     "Qwen/Qwen2.5-VL-3B-Instruct",
# #     torch_dtype=torch.bfloat16,
# #     attn_implementation="flash_attention_2",
# #     device_map="auto",
# # )

# # default processer
# processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct")

# # The default range for the number of visual tokens per image in the model is 4-16384.
# # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.
# # min_pixels = 256*28*28
# # max_pixels = 1280*28*28
# # processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)

# messages = [
#     {
#         "role": "user",
#         "content": [
#             {
#                 "type": "image",
#                 "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
#             },
#             {"type": "text", "text": "Describe this image."},
#         ],
#     }
# ]

# # Preparation for inference
# text = processor.apply_chat_template(
#     messages, tokenize=False, add_generation_prompt=True
# )
# image_inputs, video_inputs = process_vision_info(messages)
# inputs = processor(
#     text=[text],
#     images=image_inputs,
#     videos=video_inputs,
#     padding=True,
#     return_tensors="pt",
# )
# inputs = inputs.to("cuda")

# # Inference: Generation of the output
# generated_ids = model.generate(**inputs, max_new_tokens=128)
# generated_ids_trimmed = [
#     out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
# ]
# output_text = processor.batch_decode(
#     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
# )
# print(output_text)




############################################
# """
# Same with deepseek
# """

# import torch
# from transformers import AutoModelForCausalLM

# from transformers.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM
# from transformers.utils.io import load_pil_images


# # specify the path to the model
# model_path = "deepseek-ai/deepseek-vl2-small"
# vl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)
# tokenizer = vl_chat_processor.tokenizer

# vl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)
# vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()

# ## single image conversation example
# conversation = [
#     {
#         "role": "<|User|>",
#         "content": "<image>\n<|ref|>The giraffe at the back.<|/ref|>.",
#         "images": ["./images/visual_grounding.jpeg"],
#     },
#     {"role": "<|Assistant|>", "content": ""},
# ]

# ## multiple images (or in-context learning) conversation example
# # conversation = [
# #     {
# #         "role": "User",
# #         "content": "<image_placeholder>A dog wearing nothing in the foreground, "
# #                    "<image_placeholder>a dog wearing a santa hat, "
# #                    "<image_placeholder>a dog wearing a wizard outfit, and "
# #                    "<image_placeholder>what's the dog wearing?",
# #         "images": [
# #             "images/dog_a.png",
# #             "images/dog_b.png",
# #             "images/dog_c.png",
# #             "images/dog_d.png",
# #         ],
# #     },
# #     {"role": "Assistant", "content": ""}
# # ]

# # load images and prepare for inputs
# pil_images = load_pil_images(conversation)
# prepare_inputs = vl_chat_processor(
#     conversations=conversation,
#     images=pil_images,
#     force_batchify=True,
#     system_prompt=""
# ).to(vl_gpt.device)

# # run image encoder to get the image embeddings
# inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)

# # run the model to get the response
# outputs = vl_gpt.language_model.generate(
#     inputs_embeds=inputs_embeds,
#     attention_mask=prepare_inputs.attention_mask,
#     pad_token_id=tokenizer.eos_token_id,
#     bos_token_id=tokenizer.bos_token_id,
#     eos_token_id=tokenizer.eos_token_id,
#     max_new_tokens=512,
#     do_sample=False,
#     use_cache=True
# )

# answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)
# print(f"{prepare_inputs['sft_format'][0]}", answer)
