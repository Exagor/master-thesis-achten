"""
Script to evaluate prompts generated by other LLMs
Here the LLM is a scoring model that evaluates the prompts
"""

from pdf_parser import *
import os
import json
from huggingface_hub import login
from transformers import pipeline
from tqdm import tqdm
import torch

model_name = "google/gemma-3-4b-it"
model_name_shrt = "gemma3_4B"
prompt_name = "gpt4o"


#get the prompt
with open(f"prompt/generated_prompt_metadata_{prompt_name}_merged.txt", "r") as f: #to test gpt4o, gpto3 and gemini
    system_prompt_meta = f.read()
with open(f"prompt/generated_prompt_mutation_{prompt_name}_merged.txt", "r") as f:
    system_prompt_mut = f.read()

# Get example input (text+image)
pdf_folder_path = "data/PDF"
pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]
# pdf_files = ["24EM03456.pdf"]

pdf_text_image = {}
for pdf_file in pdf_files:
    pdf_text_image[os.path.splitext(pdf_file)[0]] = {"text":extract_with_pdfplumber(os.path.join(pdf_folder_path,pdf_file))}
    pdf_images = extract_pdf2image(f"{pdf_folder_path}/{pdf_file}")
    pdf_text_image[os.path.splitext(pdf_file)[0]]["image"] = pdf_images

# Login to Hugging Face to enable the use of gemma 3
with open("login_huggingface.txt", "r") as f:
    token = f.read()
try:
    login(token) #token from huggingface.co necessary to use gemma3
except Exception as e:
    print(f"Failed to login to hugging face: {e}")

# Create the model and processor
device = "cuda"

pipe = pipeline(
    "text-generation", # "image-text-to-text" or "text-generation"
    model=model_name,
    torch_dtype=torch.bfloat16,
    device=device,
    #device_map="auto", #use "auto" to automatically use all available GPUs (but slows the code ??!!)
)

metadata_data = []
mutation_data = []

# loop to process multiple pdfs
for pdf_number,pdf in tqdm(pdf_text_image.items()):

    messages_meta = [
        {
            "role": "system",
            "content": [{"type": "text", "text": system_prompt_meta}]
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": pdf["text"]}]
                # {"type":"image", "image": pdf["image"][0]}]
                + [{"type": "image", "image": img} for img in pdf["image"]]
        }
    ]

    messages_mut = [
        {
            "role": "system",
            "content": [{"type": "text", "text": system_prompt_mut}]
        },
        {
            "role": "user",
            "content": [
                {"type": "text", "text": pdf["text"]}]
                # {"type":"image", "image": pdf["image"][0]}]
                + [{"type": "image", "image": img} for img in pdf["image"]]
        }
    ]

    # Run the inference
    output_meta = pipe(text=messages_meta, max_new_tokens=250) #don't forget the text= parameter and temperature=0 doesn't work
    output_mut = pipe(text=messages_mut, max_new_tokens=550)

    # Process the output
    answer_meta = output_meta[0]["generated_text"][-1]["content"]
    cleaned_meta = answer_meta.replace("```json", "").replace("```", "").strip() #clean the answer
    try: #to handle when the answer is not a valid json
        data_meta = json.loads(cleaned_meta)
        metadata_data.append(data_meta)
    except json.JSONDecodeError as e:
        print(f"Failed to decode JSON for {pdf_number}: {e} \nContent: {cleaned_meta}")

    answer_mut = output_mut[0]["generated_text"][-1]["content"]
    cleaned_mut = answer_mut.replace("```json", "").replace("```", "").strip() #clean the answer
    try:
        data_mut = json.loads(cleaned_mut)
        mutation_data.append(data_mut)
    except json.JSONDecodeError as e:
        print(f"Failed to decode JSON for {pdf_number}: {e} \nContent: {cleaned_mut}")

df_meta = pd.DataFrame(metadata_data)
# Remove % from the '% cellules' column if it exists
if '% de cellules' in df_meta.columns:
    df_meta['% de cellules'] = df_meta['% de cellules'].astype(str).str.replace('%', '', regex=False)
df_meta.to_excel(f"out/metadata_{model_name_shrt}_{prompt_name}.xlsx", index=False)

# Flatten the mutation data
flat_mutation_data = [item for sublist in mutation_data for item in sublist]
df_mut = pd.DataFrame(flat_mutation_data)
# Remove rows with any None values
df_mut = df_mut.dropna()
df_mut.to_excel(f"out/mutation_{model_name_shrt}_{prompt_name}.xlsx", index=False)
    




######################################################
# """
# Same with qwen2.5VL

# """
# from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
# from qwen_vl_utils import process_vision_info

# # default: Load the model on the available device(s)
# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
#     "Qwen/Qwen2.5-VL-3B-Instruct", torch_dtype="auto", device_map="auto"
# )

# # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
# #     "Qwen/Qwen2.5-VL-3B-Instruct",
# #     torch_dtype=torch.bfloat16,
# #     attn_implementation="flash_attention_2",
# #     device_map="auto",
# # )

# # default processer
# processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct")

# # The default range for the number of visual tokens per image in the model is 4-16384.
# # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.
# # min_pixels = 256*28*28
# # max_pixels = 1280*28*28
# # processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)

# messages = [
#     {
#         "role": "user",
#         "content": [
#             {
#                 "type": "image",
#                 "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
#             },
#             {"type": "text", "text": "Describe this image."},
#         ],
#     }
# ]

# # Preparation for inference
# text = processor.apply_chat_template(
#     messages, tokenize=False, add_generation_prompt=True
# )
# image_inputs, video_inputs = process_vision_info(messages)
# inputs = processor(
#     text=[text],
#     images=image_inputs,
#     videos=video_inputs,
#     padding=True,
#     return_tensors="pt",
# )
# inputs = inputs.to("cuda")

# # Inference: Generation of the output
# generated_ids = model.generate(**inputs, max_new_tokens=128)
# generated_ids_trimmed = [
#     out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
# ]
# output_text = processor.batch_decode(
#     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
# )
# print(output_text)




############################################
# """
# Same with deepseek
# """

# import torch
# from transformers import AutoModelForCausalLM

# from transformers.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM
# from transformers.utils.io import load_pil_images


# # specify the path to the model
# model_path = "deepseek-ai/deepseek-vl2-small"
# vl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)
# tokenizer = vl_chat_processor.tokenizer

# vl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)
# vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()

# ## single image conversation example
# conversation = [
#     {
#         "role": "<|User|>",
#         "content": "<image>\n<|ref|>The giraffe at the back.<|/ref|>.",
#         "images": ["./images/visual_grounding.jpeg"],
#     },
#     {"role": "<|Assistant|>", "content": ""},
# ]

# ## multiple images (or in-context learning) conversation example
# # conversation = [
# #     {
# #         "role": "User",
# #         "content": "<image_placeholder>A dog wearing nothing in the foreground, "
# #                    "<image_placeholder>a dog wearing a santa hat, "
# #                    "<image_placeholder>a dog wearing a wizard outfit, and "
# #                    "<image_placeholder>what's the dog wearing?",
# #         "images": [
# #             "images/dog_a.png",
# #             "images/dog_b.png",
# #             "images/dog_c.png",
# #             "images/dog_d.png",
# #         ],
# #     },
# #     {"role": "Assistant", "content": ""}
# # ]

# # load images and prepare for inputs
# pil_images = load_pil_images(conversation)
# prepare_inputs = vl_chat_processor(
#     conversations=conversation,
#     images=pil_images,
#     force_batchify=True,
#     system_prompt=""
# ).to(vl_gpt.device)

# # run image encoder to get the image embeddings
# inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)

# # run the model to get the response
# outputs = vl_gpt.language_model.generate(
#     inputs_embeds=inputs_embeds,
#     attention_mask=prepare_inputs.attention_mask,
#     pad_token_id=tokenizer.eos_token_id,
#     bos_token_id=tokenizer.bos_token_id,
#     eos_token_id=tokenizer.eos_token_id,
#     max_new_tokens=512,
#     do_sample=False,
#     use_cache=True
# )

# answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)
# print(f"{prepare_inputs['sft_format'][0]}", answer)
