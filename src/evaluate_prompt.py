"""
Script to evaluate prompts generated by other LLMs
Here the LLM is a scoring model that evaluates the prompts
"""

from transformers import AutoProcessor, Gemma3ForConditionalGeneration
from PIL import Image
import requests
import torch
from pdf_parser import *
import os

#get the prompt
with open("prompt/generated_prompt_metadata_gpt4o_merged.txt", "r") as f: #to test gpt4o, gpto3 and gemini
    system_prompt_meta = f.read()
with open("prompt/generated_prompt_mutation_gpt4o_merged.txt", "r") as f:
    system_prompt_mut = f.read()

# Get example input (text+image)
pdf_folder_path = "../data/PDF"
pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]
# pdf_files = ["24EM03456.pdf"]

pdf_text_image = {}
for pdf_file in pdf_files:
    #could use langchain here to extract text from pdf and use Document object
    pdf_text_image[os.path.splitext(pdf_file)[0]] = {"text":extract_with_pdfplumber(os.path.join(pdf_folder_path,pdf_file))}
    pdf_images = extract_pdf2image(f"{pdf_folder_path}/{pdf_file}")
    pdf_text_image[os.path.splitext(pdf_file)[0]]["image"] = pdf_images


# Create the model and processor
model_id = "google/gemma-3-4b-it"

model = Gemma3ForConditionalGeneration.from_pretrained(
    model_id, device_map="auto"
)

processor = AutoProcessor.from_pretrained(model_id)

messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a helpful assistant."}]
    },
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg"},
            {"type": "text", "text": "Describe this image in detail."}
        ]
    }
]

inputs = processor.apply_chat_template(
    messages, add_generation_prompt=True, tokenize=True,
    return_dict=True, return_tensors="pt"
).to(model.device, dtype=torch.bfloat16)

input_len = inputs["input_ids"].shape[-1]

with torch.inference_mode(): #inference mode is similar to nograds param
    output = model.generate(**inputs, max_new_tokens=100, do_sample=False)
    output = output[0][input_len:]

decoded = processor.decode(output, skip_special_tokens=True)
print(decoded)


# Save the results to a file




######################################################
# """
# Same with qwen2.5VL

# """
# from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
# from qwen_vl_utils import process_vision_info

# # default: Load the model on the available device(s)
# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
#     "Qwen/Qwen2.5-VL-3B-Instruct", torch_dtype="auto", device_map="auto"
# )

# # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
# #     "Qwen/Qwen2.5-VL-3B-Instruct",
# #     torch_dtype=torch.bfloat16,
# #     attn_implementation="flash_attention_2",
# #     device_map="auto",
# # )

# # default processer
# processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct")

# # The default range for the number of visual tokens per image in the model is 4-16384.
# # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.
# # min_pixels = 256*28*28
# # max_pixels = 1280*28*28
# # processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)

# messages = [
#     {
#         "role": "user",
#         "content": [
#             {
#                 "type": "image",
#                 "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
#             },
#             {"type": "text", "text": "Describe this image."},
#         ],
#     }
# ]

# # Preparation for inference
# text = processor.apply_chat_template(
#     messages, tokenize=False, add_generation_prompt=True
# )
# image_inputs, video_inputs = process_vision_info(messages)
# inputs = processor(
#     text=[text],
#     images=image_inputs,
#     videos=video_inputs,
#     padding=True,
#     return_tensors="pt",
# )
# inputs = inputs.to("cuda")

# # Inference: Generation of the output
# generated_ids = model.generate(**inputs, max_new_tokens=128)
# generated_ids_trimmed = [
#     out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
# ]
# output_text = processor.batch_decode(
#     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
# )
# print(output_text)




############################################
# """
# Same with deepseek
# """

# import torch
# from transformers import AutoModelForCausalLM

# from transformers.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM
# from transformers.utils.io import load_pil_images


# # specify the path to the model
# model_path = "deepseek-ai/deepseek-vl2-small"
# vl_chat_processor: DeepseekVLV2Processor = DeepseekVLV2Processor.from_pretrained(model_path)
# tokenizer = vl_chat_processor.tokenizer

# vl_gpt: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)
# vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()

# ## single image conversation example
# conversation = [
#     {
#         "role": "<|User|>",
#         "content": "<image>\n<|ref|>The giraffe at the back.<|/ref|>.",
#         "images": ["./images/visual_grounding.jpeg"],
#     },
#     {"role": "<|Assistant|>", "content": ""},
# ]

# ## multiple images (or in-context learning) conversation example
# # conversation = [
# #     {
# #         "role": "User",
# #         "content": "<image_placeholder>A dog wearing nothing in the foreground, "
# #                    "<image_placeholder>a dog wearing a santa hat, "
# #                    "<image_placeholder>a dog wearing a wizard outfit, and "
# #                    "<image_placeholder>what's the dog wearing?",
# #         "images": [
# #             "images/dog_a.png",
# #             "images/dog_b.png",
# #             "images/dog_c.png",
# #             "images/dog_d.png",
# #         ],
# #     },
# #     {"role": "Assistant", "content": ""}
# # ]

# # load images and prepare for inputs
# pil_images = load_pil_images(conversation)
# prepare_inputs = vl_chat_processor(
#     conversations=conversation,
#     images=pil_images,
#     force_batchify=True,
#     system_prompt=""
# ).to(vl_gpt.device)

# # run image encoder to get the image embeddings
# inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)

# # run the model to get the response
# outputs = vl_gpt.language_model.generate(
#     inputs_embeds=inputs_embeds,
#     attention_mask=prepare_inputs.attention_mask,
#     pad_token_id=tokenizer.eos_token_id,
#     bos_token_id=tokenizer.bos_token_id,
#     eos_token_id=tokenizer.eos_token_id,
#     max_new_tokens=512,
#     do_sample=False,
#     use_cache=True
# )

# answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)
# print(f"{prepare_inputs['sft_format'][0]}", answer)
